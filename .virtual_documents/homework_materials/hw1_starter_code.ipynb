#@title Imports and Initialization
# %pip install datasets
# %pip install textwrap
# %pip install openai
# %pip install scipy
# %pip install matplotlib

import collections
from abc import ABC
import datasets
import json
import openai
import numpy as np
from scipy.special import softmax
import textwrap
import matplotlib.pyplot as plt
from IPython.display import clear_output

OPENAI_SECRET_KEY = None

clear_output()





if OPENAI_SECRET_KEY is None:
  print("Please paste your OpenAI API key here:")
  OPENAI_SECRET_KEY = input().strip()
openai.api_key = OPENAI_SECRET_KEY
clear_output()

class OpenAIEngine():
  def __init__(self, model_name):
    self.model_name = model_name

  def score(self, text):
    """Tokenizes and scores a piece of text.

    This only works for the OpenAI models which support the legacy `Completion`
    API.

    The score is log-likelihood. A higher score means a token was more
    likely according to the model.

    Returns a list of tokens and a list of scores.
    """
    response = openai.Completion.create(
        engine=self.model_name,
        prompt=text,
        max_tokens=0,
        logprobs=1,
        echo=True)

    tokens = response["choices"][0]["logprobs"]["tokens"]
    logprobs = response["choices"][0]["logprobs"]["token_logprobs"]
    if logprobs and logprobs[0] is None:
      # GPT-3 API does not return logprob of the first token
      logprobs[0] = 0.0
    return tokens, logprobs

  def perplexity(self, text):
    """Compute the perplexity of the provided text."""
    completion = openai.Completion.create(
        model=self.model_name,
        prompt=text,
        logprobs=0,
        max_tokens=0,
        temperature=1.0,
        echo=True)
    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']
    nll = np.mean([i for i in token_logprobs if i is not None])
    ppl = np.exp(-nll)
    return ppl

  def generate(self,
               prompt,
               top_p=1.0,
               num_tokens=32,
               num_samples=1,
               frequency_penalty=0.0,
              presence_penalty=0.0):
    """Generates text given the provided prompt text.

    This only works for the OpenAI models which support the legacy `Completion`
    API.

    If num_samples is 1, a single generated string is returned.
    If num_samples > 1, a list of num_samples generated strings is returned.
    """
    response = openai.Completion.create(
      engine=self.model_name,
      prompt=prompt,
      temperature=1.0,
      max_tokens=num_tokens,
      top_p=top_p,
      n=num_samples,
      frequency_penalty=frequency_penalty,
      presence_penalty=presence_penalty,
      logprobs=1,
    )
    outputs = [r["text"] for r in response["choices"]]
    return outputs[0] if num_samples == 1 else outputs


  def chat_generate(self,
                    previous_messages,
                    top_p=1.0,
                    num_tokens=32,
                    num_samples=1,
                    frequency_penalty=0.0,
                    presence_penalty=0.0):
    response = openai.ChatCompletion.create(
      model=self.model_name,
      messages=previous_messages,
      temperature=1.0,
      max_tokens=num_tokens,
      top_p=top_p,
      frequency_penalty=frequency_penalty,
      presence_penalty=presence_penalty,
      n=num_samples,
    )
    return response








MODEL_NAME = "davinci-002"
engine = OpenAIEngine(MODEL_NAME)


number_map = {
    "one": 1,
    "two": 2,
    "three": 3,
    "four": 4,
    "five": 5,
    "six": 6,
    "seven": 7,
    "eight": 8,
    "nine": 9,
    "ten": 10,
    "eleven": 11,
    "twelve": 12,
    "thirteen": 13,
    "fourteen": 14,
    "fifteen": 15,
    "sixteen": 16,
    "seventeen": 17,
    "eighteen": 18,
    "nineteen": 19,
    "twenty": 20
}


# prompt = "Let's roll a D20. The die shows the number"
# prompt = "Let's roll a D20, which randomly get a number from 1 to 20, the number is?"
# prompt = "Let's roll a D20, which number does this die show?"
prompt = "Let's roll a D20, the number is"
rolls = engine.generate(prompt, num_tokens=2, num_samples=128, top_p=0.2)
# rolls = engine.generate(prompt, num_samples=128, top_p=0.8)
expected_number_of_outcomes = 20


rolls_counter = collections.Counter()

for roll in rolls:
  try:
    roll_num = int(roll.strip())
    # Let's label invalid numbers as -1
    roll_num = roll_num if 1 <= roll_num <= 20 else -1
  except ValueError:
    # Let's just label invalid generation as a roll of -1.
    if (number_map.get(roll.lower().strip())):
        roll_num = number_map.get(roll.lower().strip());
    else:
        roll_num = -1
  rolls_counter[roll_num] += 1

print(rolls_counter)
print("Percentage of valid outcomes generated:",
      (len(rolls_counter)-1)/expected_number_of_outcomes)


set(rolls)


for roll in rolls:
    print(roll)


def get_valid_counter(counter):
    # Define the value to remove
    value_to_remove = -1

    # Get the frequency of the value to remove
    frequency_to_remove = counter[value_to_remove]

    # Remove the value from the Counter
    counter.subtract({value_to_remove: frequency_to_remove})

    # Remove any negative counts (optional)
    for key in list(counter):
        if counter[key] <= 0:
            del counter[key]

    return(counter)


# unique_tokens, counts = np.unique(rolls, return_counts=True)
valid_counter = get_valid_counter(rolls_counter)
valid_num, counts = zip(*valid_counter.items())

# str_num = [str(num) for num in valid_num]

plt.bar(valid_num, counts)
plt.xlabel('valid_num')
plt.ylabel('Counts')
plt.title('Distribution of the valid responses')
plt.xticks(rotation=45)
plt.xticks(range(1, 22))
plt.show()


# try random generation
import random

rand_counter = collections.Counter()

for i in range(40):
    rand = random.randint(1, 20)
    rand_counter[rand] += 1
    # print(i)

print(rand_counter)
print("Percentage of valid outcomes generated:",
      (len(rand_counter)-1)/expected_number_of_outcomes)


num, counts = zip(*rand_counter.items())

# str_num = [str(num) for num in valid_num]

plt.bar(num, counts)
plt.xlabel('Number')
plt.ylabel('Counts')
plt.title('Distribution of the Number frequency')
plt.xticks(rotation=45)
plt.xticks(range(min(valid_num), 22))
plt.show()





import math

def calculate_ttr(text):
    words = text.split()
    unique_words = set(words)
    ttr = len(unique_words) / len(words)
    return ttr

def calculate_malones_measure(text):
    words = text.split()
    unique_words = set(words)
    d = 1 - (len(unique_words) / len(words))
    return d

def calculate_herdans_c(text):
    words = text.split()
    unique_words = set(words)
    vocabulary_size = len(unique_words)
    n = len(words)
    c = vocabulary_size / math.sqrt(n)
    return c


def output_res(res):
    print(f'Response: {res}\n')
    print(f'TTR: {calculate_ttr(res)}')
    print(f'Malone\'s Measure: {calculate_malones_measure(res)}')
    print(f'Herdan\'s C: {calculate_herdans_c(res)}')





# Prompt with top_p = 0.0
prompt = "Once upon a time, there is a scientist wanting to create a time machine"
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=0.0, frequency_penalty=0.0)

output_res(res)


# Prompt with top_p = 0.5
prompt = "Once upon a time, there is a scientist wanting to create a time machine"
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=0.5, frequency_penalty=0.0)
output_res(res)


# Prompt with top_p = 1.0
prompt = "Once upon a time, there is a scientist wanting to create a time machine"
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=1.0, frequency_penalty=0.0)
output_res(res)





# prompt = "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness,"
# Prompt with top_p = 0.0
prompt = "I have a dream that one day this nation will rise up and live out the true meaning of its creed. We hold these truths to be self-evident that all men are created equal."
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=0.0, frequency_penalty=0.0)
output_res(res)


prompt = "I have a dream that one day this nation will rise up and live out the true meaning of its creed. We hold these truths to be self-evident that all men are created equal."
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=0.0, frequency_penalty=0.0)
output_res(res)


# Prompt with top_p = 0.5
prompt = "I have a dream that one day this nation will rise up and live out the true meaning of its creed. We hold these truths to be self-evident that all men are created equal."
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=0.5, frequency_penalty=0.0)
output_res(res)


# Prompt with top_p = 1.0
prompt = "I have a dream that one day this nation will rise up and live out the true meaning of its creed. We hold these truths to be self-evident that all men are created equal."
res = engine.generate(prompt, num_tokens=256, num_samples=1, top_p=1.0, frequency_penalty=0.0)
output_res(res)





MODEL_NAME = "davinci"
engine = OpenAIEngine(MODEL_NAME)


poem = """
’Twas brillig, and the slithy toves
      Did gyre and gimble in the wabe:
All mimsy were the borogoves,
      And the mome raths outgrabe.

“Beware the Jabberwock, my son!
      The jaws that bite, the claws that catch!
Beware the Jubjub bird, and shun
      The frumious Bandersnatch!”

He took his vorpal sword in hand;
      Long time the manxome foe he sought—
So rested he by the Tumtum tree
      And stood awhile in thought.

And, as in uffish thought he stood,
      The Jabberwock, with eyes of flame,
Came whiffling through the tulgey wood,
      And burbled as it came!

One, two! One, two! And through and through
      The vorpal blade went snicker-snack!
He left it dead, and with its head
      He went galumphing back.

“And hast thou slain the Jabberwock?
      Come to my arms, my beamish boy!
O frabjous day! Callooh! Callay!”
      He chortled in his joy.

’Twas brillig, and the slithy toves
      Did gyre and gimble in the wabe:
All mimsy were the borogoves,
      And the mome raths outgrabe.
"""

engine.perplexity(poem)








copa_dataset = datasets.load_dataset("super_glue", "copa")

# You may draw on these examples to produce few-shot prompts.
train_data = copa_dataset["train"].shuffle(seed=1).select(range(50))

# Use this development set to try out different few-shot prompts to see
# what works best.
dev_data = copa_dataset["train"].shuffle(seed=1).select(range(50, 150))

# You should only use this at the end during final evaluation to generate
# accuracies to put in your report.
test_data = copa_dataset["validation"].shuffle(seed=1).select(range(100))

print("Some examples from the train set:")
for i in range(3):
  print(json.dumps(train_data[i], indent=2))


prompt = "Given the following premise and cause, label whether the cause seems correct\n\n"
eval_template = "Review: {review}\nSentiment: {sentiment}"


def classify_baseline(premise: str, choice1: str, choice2:str) -> str:
  """ Given a review, returns a sentiment prediction, 0 for negative, 1 for positive."""

  eval_template = """Which of the following makes more sense?

  Choice 1: {premise} This happened because: {choice1}
  Choice 2: {premise} This happened because: {choice2}

  {label} makes more sense.
  """
  label_map = {0: "Choice 1", 1: "Choice 2"}

  label_to_score = {}
  for label, label_str in label_map.items():
    label_prompt = prompt + eval_template.format(
        premise=premise, choice1=choice1, choice2=choice2, label=label_str)
    _, score = engine.score(label_prompt)
    llm_score_for_label = np.mean(score)

    label_to_score[label] = llm_score_for_label

  return max(label_to_score, key=label_to_score.get)


def evaluate(dataset, verbose: bool=False) -> float:
  """ Evaluate your prompt on the test set """
  correct = []
  for i, instance in enumerate(dataset):
    label = instance["label"]
    predicted = classify_baseline(
        instance["premise"], instance["choice1"], instance["choice2"])
    correct.append(1 if label == predicted else 0)

    if verbose:
      print(f"======== {i+1} / {len(dataset)} ========")
      print(f"PREMISE: {instance['premise']}")
      print(f"CHOICE 1 {'✅' if not label else '❌'}: {instance['choice1']}")
      print(f"CHOICE 2 {'✅' if label else '❌'}: {instance['choice2']}")
      print(f"PREDICTED: {'choice 2' if predicted else 'choice 1'}")

  acc = sum(correct) / len(correct)
  return acc

#  Once you have chosen your prompts, for final evaluation, replace dev_data
# with test_data.
acc = evaluate(dev_data, verbose=True)
print(f"Accuracy of your prompt on {len(test_data)} test examples: {acc:.0%}")
